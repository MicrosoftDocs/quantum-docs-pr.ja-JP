---
title: クォンタム機械学習ライブラリ
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 2/27/2020
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.training
ms.openlocfilehash: f9b33a607a892179795d0700ba3080f9a24ab94a
ms.sourcegitcommit: 6ccea4a2006a47569c4e2c2cb37001e132f17476
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 02/28/2020
ms.locfileid: "77909774"
---
# <a name="quantum-machine-learning-glossary"></a>Quantum Machine Learning 用語集

回線中心のクォンタム分類器のトレーニングは、従来の分類器のトレーニングと同じ (または少し大きく) の評価による調整を必要とする多数の移動部品を含むプロセスです。 ここでは、このトレーニングプロセスの主要な概念と成分を定義します。

## <a name="trainingtesting-schedules"></a>トレーニング/テストのスケジュール

分類器トレーニングのコンテキストでは、トレーニングセットまたはテストセット全体のデータサンプルのサブセット*について説明します*。 スケジュールは、通常、サンプルインデックスのコレクションとして定義されます。

## <a name="parameterbias-scores"></a>パラメーター/バイアススコア

候補のパラメーターベクターと分類子バイアスが指定されている場合、その*検証スコア*は、選択した検証スケジュールに対して相対的に測定され、スケジュール内のすべてのサンプルに対してカウントされる誤分類の数によって表されます。

## <a name="hyperparameters"></a>ハイパーパラメーター

モデルのトレーニングプロセスは、*ハイパーパラメーター*と呼ばれる特定の事前設定された値によって管理されます。

### <a name="learning-rate"></a>学習率

これは、ハイパーパラメーターのキーの1つです。 パラメーターの更新に影響を与える現在のストキャスティクス勾配の推定量を定義します。 パラメーター更新デルタのサイズは学習速度に比例します。 学習速度の値を小さくすると、パラメーターの進化が遅くなり、収束が遅くなりますが、LR の値が非常に大きくなると、グラデーションの降下が特定のローカル最小値にコミットされないため、収束が完全に解除される可能性があります。 学習率はトレーニングアルゴリズムによって状況に応じて調整されますが、適切な初期値を選択することが重要です。 学習率の通常の既定の初期値は0.1 です。 学習率の最高値を選択することは、優れたアートです (例については、Goodfellow et al のセクション4.3、「ディープラーニング」、MIT プレス、2017)。

### <a name="minibatch-size"></a>Minibatch サイズ

ストキャスティクス勾配の単一の推定に使用されるデータサンプル数を定義します。 ミニバッチのサイズを大きくすると、一般に、より堅牢で単調な収束につながりますが、1つのグラデーション見積もりのコストは minimatch のサイズに比例しているため、トレーニングプロセスの速度が低下する可能性があります。 ミニバッチサイズの通常の既定値は10です。

### <a name="training-epochs-tolerance-gridlocks"></a>トレーニングエポック、tolerance、gridlocks

"エポック" は、スケジュールされたトレーニングデータを1回完全に通過することを意味します。
トレーニングスレッドあたりのエポックの最大数 (下記参照) は、上限にする必要があります。 トレーニングスレッドは、エポックの最大数が実行されたときに (最もよく知られている候補のパラメーターを使用して) 終了するように定義されています。 ただし、このようなトレーニングは、検証スケジュールの誤分類率が選択された許容範囲を下回ったときに終了します。 たとえば、誤った分類の許容範囲が 0.01 (1%) であるとします。2000サンプルの検証セットでは、20個未満の分類がある場合は、許容レベルが達成されていることがわかります。 また、トレーニングスレッドは、候補モデルの検証スコアに連続する複数のエポック (gridlock) の改善が示されていない場合にも、途中で終了します。 渋滞の終了のロジックは現在ハードコードされています。

### <a name="measurements-count"></a>測定数

クォンタムデバイスでのトレーニング/検証スコアとストキャスティクスグラデーションのコンポーネントを推定して、適切な observable の複数の測定値を必要とするクォンタム状態の重複を推定します。 測定の数は $O (1/\ イプシロン ^ 2) $ として拡張する必要があります。 $-イプシロン $ は目的の推定誤差です。
経験則として、初期測定数は約 $ 1/\ mbox {tolerance} ^ 2 $ (前の段落の許容範囲の定義を参照してください) になります。 グラデーションの降下率が非常に極端で、収束が難しい場合は、測定数を大きくする必要があります。

### <a name="training-threads"></a>トレーニングスレッド

分類子のトレーニングユーティリティである尤度関数は、あまり凸されないことを意味します。つまり、通常は、品質によって大きく異なる可能性のあるパラメーター空間に多くのローカル最適化があります。 SGD プロセスは1つの特定の最適化にのみ収束できるため、複数の開始パラメーターベクトルを調べることが重要です。 機械学習での一般的な方法は、このような開始ベクトルをランダムに初期化することです。 Q # トレーニング API は、このような開始ベクトルの任意の配列を受け入れますが、基になるコードはそれらを順番に検討します。 マルチコアコンピューターでは、または実際には任意の並列コンピューティングアーキテクチャにおいて、複数の Q # トレーニング API の呼び出しを、呼び出し全体で異なるパラメーター初期化と並行して実行することをお勧めします。

#### <a name="how-to-modify-the-hyperparameters"></a>ハイパーパラメーターを変更する方法

QML ライブラリでは、ハイパーパラメーターを変更する最善の方法は、UDT [`TrainingOptions`](xref:microsoft.quantum.machinelearning.trainingoptions)の既定値をオーバーライドすることです。 これを行うには、関数[`DefaultTrainingOptions`](xref:microsoft.quantum.machinelearning.defaulttrainingoptions)を使用して呼び出し、演算子 `w/` を適用して既定値をオーバーライドします。 たとえば、10万測定値を使用し、学習率0.01 を使用するには、次のようにします。
 ```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
 ```
