---
title: 量子機械学習ライブラリの用語集
description: クォンタム機械学習の用語集
author: alexeib2
ms.author: alexeib
ms.date: 2/27/2020
ms.topic: conceptual
uid: microsoft.quantum.libraries.machine-learning.training
no-loc:
- Q#
- $$v
ms.openlocfilehash: b6133c1f3068dff597f71d2111e5e117131a7fd1
ms.sourcegitcommit: 71605ea9cc630e84e7ef29027e1f0ea06299747e
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 01/26/2021
ms.locfileid: "98852714"
---
# <a name="quantum-machine-learning-glossary"></a><span data-ttu-id="230f8-103">Quantum Machine Learning 用語集</span><span class="sxs-lookup"><span data-stu-id="230f8-103">Quantum Machine Learning glossary</span></span>

<span data-ttu-id="230f8-104">回線中心のクォンタム分類器のトレーニングは、従来の分類器のトレーニングと同じ (または少し大きく) の評価による調整を必要とする多数の移動部品を含むプロセスです。</span><span class="sxs-lookup"><span data-stu-id="230f8-104">Training of a circuit-centric quantum classifier is a process with many moving parts that require the same (or slightly larger) amount of calibration by trial and error as training of traditional classifiers.</span></span> <span data-ttu-id="230f8-105">ここでは、このトレーニングプロセスの主要な概念と成分を定義します。</span><span class="sxs-lookup"><span data-stu-id="230f8-105">Here we define the main concepts and ingredients of this training process.</span></span>

## <a name="trainingtesting-schedules"></a><span data-ttu-id="230f8-106">トレーニング/テストのスケジュール</span><span class="sxs-lookup"><span data-stu-id="230f8-106">Training/testing schedules</span></span>

<span data-ttu-id="230f8-107">分類器トレーニングのコンテキストでは、トレーニングセットまたはテストセット全体のデータサンプルのサブセット *について説明します* 。</span><span class="sxs-lookup"><span data-stu-id="230f8-107">In the context of classifier training a *schedule* describes a subset of data samples in an overall training or testing set.</span></span> <span data-ttu-id="230f8-108">スケジュールは、通常、サンプルインデックスのコレクションとして定義されます。</span><span class="sxs-lookup"><span data-stu-id="230f8-108">A schedule is usually defined as a collection of sample indices.</span></span>

## <a name="parameterbias-scores"></a><span data-ttu-id="230f8-109">パラメーター/バイアススコア</span><span class="sxs-lookup"><span data-stu-id="230f8-109">Parameter/bias scores</span></span>

<span data-ttu-id="230f8-110">候補のパラメーターベクターと分類子バイアスが指定されている場合、その *検証スコア* は、選択した検証スケジュールに対して相対的に測定され、スケジュール内のすべてのサンプルに対してカウントされる誤分類の数によって表されます。</span><span class="sxs-lookup"><span data-stu-id="230f8-110">Given a candidate parameter vector and a classifier bias, their *validation score* is measured relative to a chosen validation schedule S and is expressed by a number of misclassifications counted over all the samples in the schedule S.</span></span>

## <a name="hyperparameters"></a><span data-ttu-id="230f8-111">ハイパーパラメーター</span><span class="sxs-lookup"><span data-stu-id="230f8-111">Hyperparameters</span></span>

<span data-ttu-id="230f8-112">モデルのトレーニングプロセスは、 *ハイパーパラメーター* と呼ばれる特定の事前設定された値によって管理されます。</span><span class="sxs-lookup"><span data-stu-id="230f8-112">The model training process is governed by certain pre-set values called *hyperparameters*:</span></span>

### <a name="learning-rate"></a><span data-ttu-id="230f8-113">Learning rate (学習率)</span><span class="sxs-lookup"><span data-stu-id="230f8-113">Learning rate</span></span>

<span data-ttu-id="230f8-114">これは、ハイパーパラメーターのキーの1つです。</span><span class="sxs-lookup"><span data-stu-id="230f8-114">It is one of the key hyperparameters.</span></span> <span data-ttu-id="230f8-115">パラメーターの更新に影響を与える現在のストキャスティクス勾配の推定量を定義します。</span><span class="sxs-lookup"><span data-stu-id="230f8-115">It defines how much current stochastic gradient estimate impacts the parameter update.</span></span> <span data-ttu-id="230f8-116">パラメーター更新デルタのサイズは学習速度に比例します。</span><span class="sxs-lookup"><span data-stu-id="230f8-116">The size of parameter update delta is proportional to the learning rate.</span></span> <span data-ttu-id="230f8-117">学習速度の値を小さくすると、パラメーターの進化が遅くなり、収束が遅くなりますが、LR の値が非常に大きくなると、グラデーションの降下が特定のローカル最小値にコミットされないため、収束が完全に解除される可能性があります。</span><span class="sxs-lookup"><span data-stu-id="230f8-117">Smaller learning rate values lead to slower parameter evolution and slower convergence, but excessively large values of LR may break the convergence altogether as the gradient descent never commits to a particular local minimum.</span></span> <span data-ttu-id="230f8-118">学習率はトレーニングアルゴリズムによって状況に応じて調整されますが、適切な初期値を選択することが重要です。</span><span class="sxs-lookup"><span data-stu-id="230f8-118">While learning rate is adaptively adjusted by the training algorithm to some extent, selecting a good initial value for it is important.</span></span> <span data-ttu-id="230f8-119">学習率の通常の既定の初期値は0.1 です。</span><span class="sxs-lookup"><span data-stu-id="230f8-119">A usual default initial value for learning rate is 0.1.</span></span> <span data-ttu-id="230f8-120">学習率の最高値を選択することは、優れたアートです (例については、Goodfellow et al のセクション4.3、「ディープラーニング」、MIT プレス、2017)。</span><span class="sxs-lookup"><span data-stu-id="230f8-120">Selecting the best value of learning rate is a fine art (see, for example, section 4.3 of Goodfellow et al.,"Deep learning", MIT Press, 2017).</span></span>

### <a name="minibatch-size"></a><span data-ttu-id="230f8-121">Minibatch サイズ</span><span class="sxs-lookup"><span data-stu-id="230f8-121">Minibatch size</span></span>

<span data-ttu-id="230f8-122">ストキャスティクス勾配の単一の推定に使用されるデータサンプル数を定義します。</span><span class="sxs-lookup"><span data-stu-id="230f8-122">Defines how many data samples is used for a single estimation of stochastic gradient.</span></span> <span data-ttu-id="230f8-123">ミニバッチのサイズを大きくすると、一般に、より堅牢で単調な収束につながりますが、1つのグラデーション見積もりのコストは minimatch のサイズに比例しているため、トレーニングプロセスの速度が低下する可能性があります。</span><span class="sxs-lookup"><span data-stu-id="230f8-123">Larger values of minibatch size generally lead to more robust and more monotonic convergence but can potentially slow down the training process, as the cost of any one gradient estimation is proportional to the minimatch size.</span></span> <span data-ttu-id="230f8-124">ミニバッチサイズの通常の既定値は10です。</span><span class="sxs-lookup"><span data-stu-id="230f8-124">A usual default value for the minibatch size is 10.</span></span>

### <a name="training-epochs-tolerance-gridlocks"></a><span data-ttu-id="230f8-125">トレーニングエポック、tolerance、gridlocks</span><span class="sxs-lookup"><span data-stu-id="230f8-125">Training epochs, tolerance, gridlocks</span></span>

<span data-ttu-id="230f8-126">"エポック" は、スケジュールされたトレーニングデータを1回完全に通過することを意味します。</span><span class="sxs-lookup"><span data-stu-id="230f8-126">"Epoch" means one complete pass through the scheduled training data.</span></span>
<span data-ttu-id="230f8-127">トレーニングスレッドあたりのエポックの最大数 (下記参照) は、上限にする必要があります。</span><span class="sxs-lookup"><span data-stu-id="230f8-127">The maximum number of epochs per a training thread (see below) should be capped.</span></span> <span data-ttu-id="230f8-128">トレーニングスレッドは、エポックの最大数が実行されたときに (最もよく知られている候補のパラメーターを使用して) 終了するように定義されています。</span><span class="sxs-lookup"><span data-stu-id="230f8-128">The training thread is defined to terminate (with the best known candidate parameters) when the maximum number of epochs has been run.</span></span> <span data-ttu-id="230f8-129">ただし、このようなトレーニングは、検証スケジュールの誤分類率が選択された許容範囲を下回ったときに終了します。</span><span class="sxs-lookup"><span data-stu-id="230f8-129">However such training would terminate earlier when misclassification rate on validation schedule falls below a chosen tolerance.</span></span> <span data-ttu-id="230f8-130">たとえば、誤った分類の許容範囲が 0.01 (1%) であるとします。2000サンプルの検証セットでは、20個未満の分類がある場合は、許容レベルが達成されていることがわかります。</span><span class="sxs-lookup"><span data-stu-id="230f8-130">Suppose, for example, that misclassification tolerance is 0.01 (1%); if on validation set of 2000 samples we are seeing fewer than 20 misclassifications, then the tolerance level has been achieved.</span></span> <span data-ttu-id="230f8-131">また、トレーニングスレッドは、候補モデルの検証スコアに連続する複数のエポック (gridlock) の改善が示されていない場合にも、途中で終了します。</span><span class="sxs-lookup"><span data-stu-id="230f8-131">A training thread also terminates prematurely if the validation score of the candidate model has not shown any improvement over several consecutive epochs (a gridlock).</span></span> <span data-ttu-id="230f8-132">渋滞の終了のロジックは現在ハードコードされています。</span><span class="sxs-lookup"><span data-stu-id="230f8-132">The logic for the gridlock termination is currently hardcoded.</span></span>

### <a name="measurements-count"></a><span data-ttu-id="230f8-133">測定数</span><span class="sxs-lookup"><span data-stu-id="230f8-133">Measurements count</span></span>

<span data-ttu-id="230f8-134">クォンタムデバイスでのトレーニング/検証スコアとストキャスティクスグラデーションのコンポーネントを推定して、適切な observable の複数の測定値を必要とするクォンタム状態の重複を推定します。</span><span class="sxs-lookup"><span data-stu-id="230f8-134">Estimating the training/validation scores and the components of the stochastic gradient on a quantum device amounts to estimating quantum state overlaps that requires multiple measurements of the appropriate observables.</span></span> <span data-ttu-id="230f8-135">測定の数は $O (1/\ イプシロン ^ 2) $ として拡張する必要があります。 $-イプシロン $ は目的の推定誤差です。</span><span class="sxs-lookup"><span data-stu-id="230f8-135">The number of measurements should scale as $O(1/\epsilon^2)$ where $\epsilon$ is the desired estimation error.</span></span>
<span data-ttu-id="230f8-136">経験則として、初期測定数は約 $ 1/\ mbox {tolerance} ^ 2 $ (前の段落の許容範囲の定義を参照してください) になります。</span><span class="sxs-lookup"><span data-stu-id="230f8-136">As a rule of thumb, the initial measurements count could be approximately $1/\mbox{tolerance}^2$ (see definition of tolerance in the previous paragraph).</span></span> <span data-ttu-id="230f8-137">グラデーションの降下率が非常に極端で、収束が難しい場合は、測定数を大きくする必要があります。</span><span class="sxs-lookup"><span data-stu-id="230f8-137">One would need to revise the measurement count upward if the gradient descent appears to be too erratic and convergence too hard to achieve.</span></span>

### <a name="training-threads"></a><span data-ttu-id="230f8-138">トレーニングスレッド</span><span class="sxs-lookup"><span data-stu-id="230f8-138">Training threads</span></span>

<span data-ttu-id="230f8-139">分類子のトレーニングユーティリティである尤度関数は、あまり凸されないことを意味します。つまり、通常は、品質によって大きく異なる可能性のあるパラメーター空間に多くのローカル最適化があります。</span><span class="sxs-lookup"><span data-stu-id="230f8-139">The likelihood function which is the training utility for the classifier is very seldom convex, meaning that it usually has a multitude of local optima in the parameter space that may differ significantly by quality.</span></span> <span data-ttu-id="230f8-140">SGD プロセスは1つの特定の最適化にのみ収束できるため、複数の開始パラメーターベクトルを調べることが重要です。</span><span class="sxs-lookup"><span data-stu-id="230f8-140">Since the SGD process can converge to only one specific optimum, it is important to explore multiple starting parameter vectors.</span></span> <span data-ttu-id="230f8-141">機械学習での一般的な方法は、このような開始ベクトルをランダムに初期化することです。</span><span class="sxs-lookup"><span data-stu-id="230f8-141">Common practice in machine learning is to initialize such starting vectors randomly.</span></span> <span data-ttu-id="230f8-142">Q#トレーニング API は、このような開始ベクトルの任意の配列を受け入れますが、基になるコードはそれらを順番に調べます。</span><span class="sxs-lookup"><span data-stu-id="230f8-142">The Q# training API accepts an arbitrary array of such starting vectors but the underlying code explores them sequentially.</span></span> <span data-ttu-id="230f8-143">マルチコアコンピューターでは、または実際には任意の並列コンピューティングアーキテクチャで、複数の Q# トレーニング API の呼び出しを、呼び出し全体で異なるパラメーター初期化と並行して実行することをお勧めします。</span><span class="sxs-lookup"><span data-stu-id="230f8-143">On a multicore computer or in fact on any parallel computing architecture it is advisable to perform several calls to Q# training API in parallel with different parameter initializations across the calls.</span></span>

#### <a name="how-to-modify-the-hyperparameters"></a><span data-ttu-id="230f8-144">ハイパーパラメーターを変更する方法</span><span class="sxs-lookup"><span data-stu-id="230f8-144">How to modify the hyperparameters</span></span>

<span data-ttu-id="230f8-145">QML ライブラリでは、ハイパーパラメーターを変更する最善の方法は、UDT の既定値をオーバーライドすることです [`TrainingOptions`](xref:Microsoft.Quantum.MachineLearning.TrainingOptions) 。</span><span class="sxs-lookup"><span data-stu-id="230f8-145">In the QML library, the best way to modify the hyperparameters is by overriding the default values of the UDT [`TrainingOptions`](xref:Microsoft.Quantum.MachineLearning.TrainingOptions).</span></span> <span data-ttu-id="230f8-146">これを行うには、関数を使用して呼び出し、 [`DefaultTrainingOptions`](xref:Microsoft.Quantum.MachineLearning.DefaultTrainingOptions) 既定値をオーバーライドする演算子を適用し `w/` ます。</span><span class="sxs-lookup"><span data-stu-id="230f8-146">To do this we call it with the function [`DefaultTrainingOptions`](xref:Microsoft.Quantum.MachineLearning.DefaultTrainingOptions) and apply the operator `w/` to override the default values.</span></span> <span data-ttu-id="230f8-147">たとえば、10万測定値を使用し、学習率0.01 を使用するには、次のようにします。</span><span class="sxs-lookup"><span data-stu-id="230f8-147">For example, to use 100,000 measurements and a learning rate of 0.01:</span></span>

```qsharp
let options = DefaultTrainingOptions()
w/ LearningRate <- 0.01
w/ NMeasurements <- 100000;
```
