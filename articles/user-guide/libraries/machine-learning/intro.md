---
title: Quantum Machine Learning Library
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 11/22/2019
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: 65b0aa6a7f385765933d4d89ce34901f77cf76ec
ms.sourcegitcommit: 75c4edc7c410cc63dc8352e2a5bef44b433ed188
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/25/2020
ms.locfileid: "88863103"
---
# <a name="introduction-to-quantum-machine-learning"></a><span data-ttu-id="4d53f-102">クォンタム Machine Learning の概要</span><span class="sxs-lookup"><span data-stu-id="4d53f-102">Introduction to Quantum Machine Learning</span></span>

## <a name="framework-and-goals"></a><span data-ttu-id="4d53f-103">フレームワークと目標</span><span class="sxs-lookup"><span data-stu-id="4d53f-103">Framework and goals</span></span>

<span data-ttu-id="4d53f-104">クォンタムのエンコードと情報の処理は、従来の機械学習の量子分類子の強力な手段です。</span><span class="sxs-lookup"><span data-stu-id="4d53f-104">Quantum encoding and processing of information is a powerful alternative to classical machine learning Quantum classifiers.</span></span> <span data-ttu-id="4d53f-105">特に、クォンタムレジスタのデータを特徴の数に対して簡潔にエンコードし、量子化を計算リソースとして使用し、クラスの推定のためにクォンタムの測定値を採用することができます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-105">In particular, it allows us to encode data in quantum registers that are concise relative to the number of features, systematically employing quantum entanglement as computational resource and employing quantum measurement for class inference.</span></span>
<span data-ttu-id="4d53f-106">サーキット中心のクォンタム分類器は、データのエンコードと高速な entangling/disentangling のクォンタム回線を結合した後、測定を使用してデータサンプルのクラスラベルを推定する、比較的単純なクォンタムソリューションです。</span><span class="sxs-lookup"><span data-stu-id="4d53f-106">Circuit centric quantum classifier is a relatively simple quantum solution that combines data encoding with a rapidly entangling/disentangling quantum circuit followed by measurement to infer class labels of data samples.</span></span>
<span data-ttu-id="4d53f-107">この目標は、非常に大規模な機能空間であっても、対象となる回線の特性とストレージを従来のように確保し、サーキットパラメーターのハイブリッド量子/古典トレーニングを行うことです。</span><span class="sxs-lookup"><span data-stu-id="4d53f-107">The goal is to ensure classical characterization and storage of subject circuits, as well as hybrid quantum/classical training of the circuit parameters even for extremely large feature spaces.</span></span>

## <a name="classifier-architecture"></a><span data-ttu-id="4d53f-108">分類子のアーキテクチャ</span><span class="sxs-lookup"><span data-stu-id="4d53f-108">Classifier architecture</span></span>

<span data-ttu-id="4d53f-109">分類は、監視対象の機械学習タスクであり、 \{ 特定のデータサンプルのクラスラベル $ y_1、y_2、\ 点線、y_d $ を推論することを目標としてい \} ます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-109">Classification is a supervised machine learning task, where the goal is to infer class labels $\{y_1,y_2,\ldots,y_d\}$ of certain data samples.</span></span> <span data-ttu-id="4d53f-110">"トレーニングデータセット" は、 \{ 既知の事前割り当て済みラベルを含むサンプル $ \mathcal{D} = (x, y)} $ のコレクションです。</span><span class="sxs-lookup"><span data-stu-id="4d53f-110">The "training data set" is a collection of samples $\mathcal{D}=\{(x,y)}$ with known pre-assigned labels.</span></span> <span data-ttu-id="4d53f-111">ここで $x $ はデータ $y サンプルであり、$ は "トレーニングラベル" と呼ばれる既知のラベルです。</span><span class="sxs-lookup"><span data-stu-id="4d53f-111">Here $x$ is a data sample and $y$ is its known label called "training label".</span></span>
<span data-ttu-id="4d53f-112">従来の方法とは少し似ていますが、クォンタム分類は次の3つの手順で構成されます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-112">Somewhat similar to traditional methods, quantum classification consists of three steps:</span></span>
- <span data-ttu-id="4d53f-113">データのエンコード</span><span class="sxs-lookup"><span data-stu-id="4d53f-113">data encoding</span></span>
- <span data-ttu-id="4d53f-114">分類子の状態の準備</span><span class="sxs-lookup"><span data-stu-id="4d53f-114">preparation of a classifier state</span></span>
- <span data-ttu-id="4d53f-115">測定値が確率論的になるため、この3つの手順を複数回繰り返す必要があります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-115">measurement Due to the probabilistic nature of the measurement, these three steps must be repeated multiple times.</span></span> <span data-ttu-id="4d53f-116">エンコーディングと分類子の状態の計算は、どちらも *クォンタム回線*を介して行われます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-116">Both the encoding and the computing of the classifier state are done by means of *quantum circuits*.</span></span> <span data-ttu-id="4d53f-117">エンコーディング回線は通常、データドリブンで、パラメーター化されていませんが、分類器回線には、learnable パラメーターの十分なセットが含まれています。</span><span class="sxs-lookup"><span data-stu-id="4d53f-117">While the encoding circuit is usually data-driven and parameter-free, the classifier circuit contains a sufficient set of learnable parameters.</span></span> 

<span data-ttu-id="4d53f-118">提案されたソリューションでは、分類器回線は、単一の qubit 回転と2つのビット制御の回転で構成されます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-118">In the proposed solution the classifier circuit is composed of single-qubit rotations and two-qubit controlled rotations.</span></span> <span data-ttu-id="4d53f-119">Learnable パラメーターは、回転角度です。</span><span class="sxs-lookup"><span data-stu-id="4d53f-119">The learnable parameters here are the rotation angles.</span></span> <span data-ttu-id="4d53f-120">回転および制御された回転ゲートは、クォンタムの計算では *ユニバーサル* であることがわかっています。つまり、すべてのユニタリウェイトの行列は、このようなゲートで構成される十分な長さの回線に分解できます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-120">The rotation and controlled rotation gates are known to be *universal* for quantum computation, which means that any unitary weight matrix can be decomposed into a long enough circuit consisting of such gates.</span></span>

<span data-ttu-id="4d53f-121">提案されたバージョンでは、1つの回線の後に1つの頻度の推定が続きます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-121">In the proposed version, only one circuit followed by a single frequency estimation is supported.</span></span>
<span data-ttu-id="4d53f-122">そのため、このソリューションは、低レベルの多項式カーネルを使用したサポートベクターマシンの量子アナログです。</span><span class="sxs-lookup"><span data-stu-id="4d53f-122">Thus, the solution is a quantum analog of a support vector machine with a low-degree polynomial kernel.</span></span>

![多層パーセプトロンとサーキット中心の分類器](~/media/DLvsQCC.png)

<span data-ttu-id="4d53f-124">単純なクォンタム分類器の設計は、従来のサポートベクターマシン (SVM) ソリューションと比較できます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-124">A simple quantum classifier design can be compared to a traditional support vector machine (SVM) solution.</span></span> <span data-ttu-id="4d53f-125">SVM の場合 $x $ \ sum \ alpha_j k (x_j, x) $ というデータサンプルの推定値が使用されます。ここで $k $ は特定のカーネル関数です。</span><span class="sxs-lookup"><span data-stu-id="4d53f-125">The inference for a data sample $x$ in case of SVM is done using an optimal kernel form $\sum \alpha_j  k(x_j,x)$ where $k$ is a certain kernel function.</span></span>

<span data-ttu-id="4d53f-126">これに対して、クォンタム分類子では、予測 $p (y │ x, U (& シータ)) = 〈 U (\ シータ) x | を使用します。M |U (\ シータ) x 〉 $ は、スピリットで似ていますが、技術的にはまったく異なります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-126">By contrast, a quantum classifier uses the predictor $p(y│x,U(\theta))=〈U(\theta)x|M|U(\theta)x〉$, which is similar in spirit but technically quite different.</span></span> <span data-ttu-id="4d53f-127">したがって、単純な振幅エンコードが使用されている場合、$p (y │ x, U (-シータ)) $ は $x $ の振幅の2次形式ですが、この形式の係数は独立して学習されなくなりました。代わりに、サーキット $U (\ シータ) $ のマトリックス要素から集計されます。この場合、通常、ベクトル $x $ の次元よりも learnable パラメーター $/シータ $ が大幅に減少します。</span><span class="sxs-lookup"><span data-stu-id="4d53f-127">Thus, when a straightforward amplitude encoding is used,  $p(y│x,U(\theta))$ is a quadratic form in the amplitudes of $x$, but the coefficients of this form are no longer learned independently; they are instead aggregated from the matrix elements of the circuit $U(\theta)$, which typically has significantly fewer learnable parameters $\theta$ than the dimension of the vector $x$.</span></span> <span data-ttu-id="4d53f-128">元の特徴の $p (y │ x, U (-シータ)) $ の多項式次数は、$x $ の $l $ コピーでクォンタム製品エンコードを使用して、$ 2 ^ l $ に増やすことができます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-128">The polynomial degree of $p(y│x,U(\theta))$ in the original features can be increased to $2^l$ by using a quantum product encoding on $l$ copies of $x$.</span></span>

<span data-ttu-id="4d53f-129">このアーキテクチャでは比較的浅い回線が検討されています。そのため、すべての範囲のデータ機能間のすべての相関関係をキャプチャするために、 *迅速に entangling* する必要があります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-129">Our architecture explores relatively shallow circuits, which therefore must be *rapidly entangling* in order to capture all the correlations between the data features at all ranges.</span></span> <span data-ttu-id="4d53f-130">次の図に、最も役に立つ entangling サーキットコンポーネントの例を示します。</span><span class="sxs-lookup"><span data-stu-id="4d53f-130">An example of the most useful rapidly entangling circuit component is shown on figure below.</span></span> <span data-ttu-id="4d53f-131">このジオメトリを持つ回線は、$3 n + 1 $ ゲートのみで構成されていますが、計算されるユニタリ weight 行列によって、$ 2 ^ n $ の特徴間で重要な相互通信が行われます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-131">Even though a circuit with this geometry consists of only $3 n+1$ gates, the unitary weight matrix that it computes ensures significant cross-talk between $2^n$ features.</span></span>

![(2 つの循環レイヤーがある) 5 つの qubits 上で、entangling のクォンタム回線を高速にします。](~/media/5-qubit-qccc.png)

<span data-ttu-id="4d53f-133">上の例のサーキットは6つのシングル qubit ゲート $ (G_1、\ lドット、G_5 で構成されています。G_ {16} ) $ および 10 2-qubits ゲート $ (G_6、\ lドット、G_ {15} ) $。</span><span class="sxs-lookup"><span data-stu-id="4d53f-133">The circuit in the above example consists of 6 single-qubit gates $(G_1,\ldots,G_5; G_{16})$ and 10 two-qubits gates $(G_6,\ldots,G_{15})$.</span></span> <span data-ttu-id="4d53f-134">各ゲートが1つの learnable パラメーターを使用して定義されていると仮定すると、16 learnable パラメーターがありますが、5 qubit ヒルベルト space の次元は32です。</span><span class="sxs-lookup"><span data-stu-id="4d53f-134">Assuming that each of the gates is defined with one learnable parameter we have 16 learnable parameters, while the dimension of the 5-qubit Hilbert space is 32.</span></span> <span data-ttu-id="4d53f-135">このようなサーキットジオメトリは任意の $n $-qubit レジスタに簡単に一般化できます。 $n $ が奇数の場合、$ 2 ^ n $ 次元の特徴空間の $3 n + 1 $ パラメーターを使用して回線を生成します。</span><span class="sxs-lookup"><span data-stu-id="4d53f-135">Such circuit geometry can be easily generalized to any $n$-qubit register, when $n$ is odd, yielding circuits with $3 n+1$ parameters for $2^n$-dimensional feature space.</span></span>

## <a name="classifier-training-as-a-supervised-learning-task"></a><span data-ttu-id="4d53f-136">監視学習タスクとしての分類器トレーニング</span><span class="sxs-lookup"><span data-stu-id="4d53f-136">Classifier training as a supervised learning task</span></span>

<span data-ttu-id="4d53f-137">分類器モデルのトレーニングでは、その操作パラメーターの最適な値が検索されます。これにより、トレーニングサンプル全体で正しいトレーニングラベルが推定される確率が最大になります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-137">Training of a classifier model involves finding optimal values of its operational parameters, such that they maximize the average likelihood of inferring the correct training labels across the training samples.</span></span>
<span data-ttu-id="4d53f-138">ここでは、2つのレベルの分類のみを考慮しています。つまり、$d = $2 の場合は、ラベルが _1 $y _1、y_2 $ の2つのクラスのみです。</span><span class="sxs-lookup"><span data-stu-id="4d53f-138">Here, we concern ourselves with two level classification only, i.e. the case of $d=2$ and only two classes with the labels $y_1,y_2$.</span></span>

> [!NOTE]
> <span data-ttu-id="4d53f-139">任意の数のクラスに対してメソッドを一般化する約束の方法として、qubits を qudits に置き換えます。つまり、$d $ basis 状態のクォンタム単位と、$d $ ウェイの測定値を持つ双方向の測定値を置き換えます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-139">A principled way of generalizing our methods to arbitrary number of classes is to replace qubits with qudits, i.e. quantum units with $d$ basis states, and the two-way measurement with $d$-way measurement.</span></span>

### <a name="likelihood-as-the-training-goal"></a><span data-ttu-id="4d53f-140">トレーニング目標としての確率</span><span class="sxs-lookup"><span data-stu-id="4d53f-140">Likelihood as the training goal</span></span>

<span data-ttu-id="4d53f-141">Learnable クォンタム回線 $U (\ シータ) $ と指定した場合、$/シータ $ はパラメーターのベクトルで、$M $ によって最終的な測定値を示します。正しいラベル推定の平均確率は $ $ \begin{align} \mathcal{L} (\ シータ) = {1} (\ sum_ {(x, y_1) \In\mathcal{D}} P (M = y_1 | です。U (\ シータ) x) + \ sum_ {(x, y_2) \in\mathcal{D}} P (M = y_2 |U (& シータ) x) \ right) \end{align} $ $ where $P (M = y | z) $ は、クォンタム状態 $z $ の $y $ を測定する確率です。</span><span class="sxs-lookup"><span data-stu-id="4d53f-141">Given a learnable quantum circuit $U(\theta)$, where $\theta$ is a vector of parameters, and denoting the final measurement by $M$, the average likelihood of the correct label inference is $$ \begin{align} \mathcal{L}(\theta)=\frac{1}{|\mathcal{D}|} \left( \sum_{(x,y_1)\in\mathcal{D}} P(M=y_1|U(\theta) x) + \sum_{(x,y_2)\in\mathcal{D}} P(M=y_2|U(\theta) x)\right) \end{align} $$ where $P(M=y|z)$ is the probability of measuring $y$ in quantum state $z$.</span></span>
<span data-ttu-id="4d53f-142">ここでは、尤度関数 $ \mathcal{L} (\ シータ) $ が $/シータ $ に smooth、$ \ theta_j $ の派生物が、実質的に尤度関数自体を計算するために使用するのと同じクォンタムプロトコルによって計算されることを理解しておく必要があります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-142">Here, it suffices to understand that the likelihood function $\mathcal{L}(\theta)$ is smooth in $\theta$ and its derivative in any $\theta_j$ can be computed by essentially the same quantum protocol as used for computing the likelihood function itself.</span></span> <span data-ttu-id="4d53f-143">これにより、グラデーション降下による $ \mathcal{L} (\ シータ) $ の最適化が可能になります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-143">This allows for optimizing the $\mathcal{L}(\theta)$ by gradient descent.</span></span>

### <a name="classifier-bias-and-training-score"></a><span data-ttu-id="4d53f-144">分類器バイアスとトレーニングスコア</span><span class="sxs-lookup"><span data-stu-id="4d53f-144">Classifier bias and training score</span></span>

<span data-ttu-id="4d53f-145">$-シータ $ 内のパラメーターの中間 (または最終) 値を指定する場合は、推定を行うために、$ $b を *分類子バイアス* として1つの実数値として指定する必要があります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-145">Given some intermediate (or final) values of the parameters in $\theta$, we need to identify a single real value $b$ know as *classifier bias* to do the inference.</span></span> <span data-ttu-id="4d53f-146">ラベルの推定規則は、次のように動作します。</span><span class="sxs-lookup"><span data-stu-id="4d53f-146">The label inference rule works as follows:</span></span> 
- <span data-ttu-id="4d53f-147">サンプル $x $ には、$P の場合にのみ、ラベル $y _2 $ が割り当てられます (M = y_2 |U (\ シータ) x) + b > $0.5 (RULE1) (それ以外の場合は、ラベル $y _1 $) が割り当てられます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-147">A sample $x$ is assigned label $y_2$ if and only if $P(M=y_2|U(\theta) x) + b > 0.5$  (RULE1) (otherwise it is assigned label $y_1$)</span></span>

<span data-ttu-id="4d53f-148">明確に $b $ は、意味のある $ (-0.5, + 0.5) $ の範囲内である必要があります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-148">Clearly $b$ must be in the interval $(-0.5,+0.5)$ to be meaningful.</span></span>

<span data-ttu-id="4d53f-149">\Mathcal{D} $ のトレーニングケース $ (x, y) \ は、RULE1 による $ として $x 推論されるラベルが、実際には $y $ とは異なる場合に $b $ で *間違った分類* と見なされます。</span><span class="sxs-lookup"><span data-stu-id="4d53f-149">A training case $(x,y) \in \mathcal{D}$ is considered a *misclassification* given the bias $b$ if the label inferred for $x$ as per RULE1 is actually different from $y$.</span></span> <span data-ttu-id="4d53f-150">誤分類の全体的な数は、分類器の *トレーニングスコア* であり、バイアス $b $ になります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-150">The overall number of misclassifications is the *training score* of the classifier given the bias $b$.</span></span> <span data-ttu-id="4d53f-151">*最適*な分類子バイアス $b $ は、トレーニングスコアを最小化します。</span><span class="sxs-lookup"><span data-stu-id="4d53f-151">The *optimal* classifier bias $b$ minimizes the training score.</span></span> <span data-ttu-id="4d53f-152">事前計算済み確率推定値 $ \{ P (M = y_2 | を指定すると、これを簡単に確認できます。U (\ シータ) x) |(x, \*) \in\mathcal{D} \} $,、最大 $ \ log_2 (| \mathcal{D} |) を作成することによって、間隔 $ (-0.5, + 0.5) $ のバイナリ検索で最適な分類子バイアスを見つけることができます。$ steps。</span><span class="sxs-lookup"><span data-stu-id="4d53f-152">It is easy to see that, given the precomputed probability estimates $\{ P(M=y_2|U(\theta) x) | (x,\*)\in\mathcal{D} \}$, the optimal classifier bias can be found by binary search in interval $(-0.5,+0.5)$ by making at most $\log_2(|\mathcal{D}|)$ steps.</span></span>

### <a name="reference"></a><span data-ttu-id="4d53f-153">関連項目</span><span class="sxs-lookup"><span data-stu-id="4d53f-153">Reference</span></span>

<span data-ttu-id="4d53f-154">この情報は、コードの再生を開始するのに十分なものである必要があります。</span><span class="sxs-lookup"><span data-stu-id="4d53f-154">This information should be enough to start playing with the code.</span></span> <span data-ttu-id="4d53f-155">ただし、このモデルの詳細については、 [ *「回線中心のクォンタム分類子」、「Alex Bocharov、Krysta svore、および Nathan Wiebe」* の提案をお読みください。](https://arxiv.org/abs/1804.00633)</span><span class="sxs-lookup"><span data-stu-id="4d53f-155">However, if you want to learn more about this model, please read the original proposal: [*'Circuit-centric quantum classifiers', Maria Schuld, Alex Bocharov, Krysta Svore and Nathan Wiebe*](https://arxiv.org/abs/1804.00633)</span></span>

<span data-ttu-id="4d53f-156">次の手順で示すコードサンプルに加えて、[このチュートリアル](https://github.com/microsoft/QuantumKatas/tree/master/tutorials/QuantumClassification)の「クォンタム分類」も参照してください。</span><span class="sxs-lookup"><span data-stu-id="4d53f-156">In addition to the code sample you will see in the next steps, you can also start exploring quantum classification in [this tutorial](https://github.com/microsoft/QuantumKatas/tree/master/tutorials/QuantumClassification)</span></span> 
