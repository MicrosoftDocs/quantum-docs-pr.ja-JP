---
title: Quantum Machine Learning Library
author: alexeib2
ms.author: alexei.bocharov@microsoft.com
ms.date: 11/22/2019
ms.topic: article
uid: microsoft.quantum.libraries.machine-learning.intro
no-loc:
- Q#
- $$v
ms.openlocfilehash: 9a24d0b4145d0db2fd8c4e16be807165fff5fb32
ms.sourcegitcommit: 6bf99d93590d6aa80490e88f2fd74dbbee8e0371
ms.translationtype: MT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/06/2020
ms.locfileid: "87868918"
---
# <a name="introduction-to-quantum-machine-learning"></a>クォンタム Machine Learning の概要

## <a name="framework-and-goals"></a>フレームワークと目標

クォンタムエンコードと情報の処理は、従来の機械学習のクォンタム分類子の強力な手段です。特に、特徴の数に対して簡潔なクォンタムレジスタ内のデータをエンコードし、量子化を計算リソースとして使用し、クラスの推定にクォンタム測定を採用します。
サーキット中心のクォンタム分類器は、データのエンコードと高速な entangling/disentangling のクォンタム回線を結合した後、測定を使用してデータサンプルのクラスラベルを推定する、比較的単純なクォンタムソリューションです。
この目標は、非常に大規模な機能空間であっても、対象となる回線の特性とストレージを従来のように確保し、サーキットパラメーターのハイブリッド量子/古典トレーニングを行うことです。

## <a name="classifier-architecture"></a>分類子のアーキテクチャ

分類は、監視対象の機械学習タスクであり、 \{ 特定のデータサンプルのクラスラベル $ y_1、y_2、\ 点線、y_d $ を推論することを目標としてい \} ます。 "トレーニングデータセット" は、 \{ 既知の事前割り当て済みラベルを含むサンプル $ \mathcal{D} = (x, y)} $ のコレクションです。 ここで $x $ はデータ $y サンプルであり、$ は "トレーニングラベル" と呼ばれる既知のラベルです。
従来の方法とは少し似ていますが、クォンタム分類は次の3つの手順で構成されます。
- データのエンコード
- 分類子の状態の準備
- 測定値が確率論的になるため、この3つの手順を複数回繰り返す必要があります。 測定値は、非線形アクティブ化と同等のクォンタムとして表示される場合があります。
エンコーディングと分類子の状態の計算は、どちらも*クォンタム回線*を介して行われます。 エンコーディング回線は通常、データドリブンで、パラメーター化されていませんが、分類器回線には、learnable パラメーターの十分なセットが含まれています。 

提案されたソリューションでは、分類器回線は、単一の qubit 回転と2つのビット制御の回転で構成されます。 Learnable パラメーターは、回転角度です。 回転および制御された回転ゲートは、クォンタムの計算では*ユニバーサル*であることがわかっています。つまり、すべてのユニタリウェイトの行列は、このようなゲートで構成される十分な長さの回線に分解できます。

![多層パーセプトロンとサーキット中心の分類器](~/media/DLvsQCC.png)

このモデルを多層パーセプトロンと比較して、基本的な構造について理解を深めることができます。 パーセプトロンでは、予測 $p (y | x,/シータ) $ はパラメーター化の組によって決定され、非線形アクティブ化関数 (neurons) を接続する線形関数を決定します。 これらのパラメーターをトレーニングして、モデルを作成することができます。 出力層では、softmax などの非線形アクティブ化関数を使用して、クラスに属するサンプルの確率を取得できます。 サーキット中心の分類器では、予測は、モデル回線の単一の qubit と2つのパラメーター化の回転角度によって決まります。 同様に、これらのパラメーターは、勾配降下アルゴリズムのハイブリッドクォンタム/クラシックバージョンでトレーニングできます。 線形的なアクティブ化関数を使用するのではなく、出力を計算するには、制御された回転の後、特定の qubit に対して繰り返しの測定値を読み取って、クラスの確率を取得します。 クォンタム状態の典型的なデータをエンコードするには、状態の準備に制御可能なエンコード回線を使用します。

このアーキテクチャでは比較的浅い回線が検討されています。そのため、すべての範囲のデータ機能間のすべての相関関係をキャプチャするために、*迅速に entangling*する必要があります。 次の図に、最も役に立つ entangling サーキットコンポーネントの例を示します。 このジオメトリを持つ回線は、$3 n + 1 $ ゲートのみで構成されていますが、計算されるユニタリ weight 行列によって、$ 2 ^ n $ の特徴間で重要な相互通信が行われます。

![(2 つの循環レイヤーがある) 5 つの qubits 上で、entangling のクォンタム回線を高速にします。](~/media/5-qubit-qccc.png)

上の例のサーキットは6つのシングル qubit ゲート $ (G_1、\ lドット、G_5 で構成されています。G_ {16} ) $ および 10 2-qubits ゲート $ (G_6、\ lドット、G_ {15} ) $。 各ゲートが1つの learnable パラメーターを使用して定義されていると仮定すると、16 learnable パラメーターがありますが、5 qubit ヒルベルト space の次元は32です。 このようなサーキットジオメトリは任意の $n $-qubit レジスタに簡単に一般化できます。 $n $ が奇数の場合、$ 2 ^ n $ 次元の特徴空間の $3 n + 1 $ パラメーターを使用して回線を生成します。

## <a name="classifier-training-as-a-supervised-learning-task"></a>監視学習タスクとしての分類器トレーニング

分類器モデルのトレーニングでは、その操作パラメーターの最適な値が検索されます。これにより、トレーニングサンプル全体で正しいトレーニングラベルが推定される確率が最大になります。
ここでは、2つのレベルの分類のみを考慮しています。つまり、$d = $2 の場合は、ラベルが _1 $y _1、y_2 $ の2つのクラスのみです。

> [!NOTE]
> 任意の数のクラスに対してメソッドを一般化する約束の方法として、qubits を qudits に置き換えます。つまり、$d $ basis 状態のクォンタム単位と、$d $ ウェイの測定値を持つ双方向の測定値を置き換えます。

### <a name="likelihood-as-the-training-goal"></a>トレーニング目標としての確率

Learnable クォンタム回線 $U (\ シータ) $ と指定した場合、$/シータ $ はパラメーターのベクトルで、$M $ によって最終的な測定値を示します。正しいラベル推定の平均確率は $ $ \begin{align} \mathcal{L} (\ シータ) = {1} (\ sum_ {(x, y_1) \In\mathcal{D}} P (M = y_1 | です。U (\ シータ) x) + \ sum_ {(x, y_2) \in\mathcal{D}} P (M = y_2 |U (& シータ) x) \ right) \end{align} $ $ where $P (M = y | z) $ は、クォンタム状態 $z $ の $y $ を測定する確率です。
ここでは、尤度関数 $ \mathcal{L} (\ シータ) $ が $/シータ $ に smooth、$ \ theta_j $ の派生物が、実質的に尤度関数自体を計算するために使用するのと同じクォンタムプロトコルによって計算されることを理解しておく必要があります。 これにより、グラデーション降下による $ \mathcal{L} (\ シータ) $ の最適化が可能になります。

### <a name="classifier-bias-and-training-score"></a>分類器バイアスとトレーニングスコア

$-シータ $ 内のパラメーターの中間 (または最終) 値を指定する場合は、推定を行うために、$ $b を*分類子バイアス*として1つの実数値として指定する必要があります。 ラベルの推定規則は、次のように動作します。 
- サンプル $x $ には、$P の場合にのみ、ラベル $y _2 $ が割り当てられます (M = y_2 |U (\ シータ) x) + b > $0.5 (RULE1) (それ以外の場合は、ラベル $y _1 $) が割り当てられます。

明確に $b $ は、意味のある $ (-0.5, + 0.5) $ の範囲内である必要があります。

\Mathcal{D} $ のトレーニングケース $ (x, y) \ は、RULE1 による $ として $x 推論されるラベルが、実際には $y $ とは異なる場合に $b $ で*間違った分類*と見なされます。 誤分類の全体的な数は、分類器の*トレーニングスコア*であり、バイアス $b $ になります。 *最適*な分類子バイアス $b $ は、トレーニングスコアを最小化します。 事前計算済み確率推定値 $ \{ P (M = y_2 | を指定すると、これを簡単に確認できます。U (\ シータ) x) |(x, *) \in\mathcal{D} \} $,、最大 $ \ log_2 (| \mathcal{D} |) を作成することによって、間隔 $ (-0.5, + 0.5) $ のバイナリ検索で最適な分類子バイアスを見つけることができます。$ steps。

### <a name="reference"></a>リファレンス

この情報は、コードの再生を開始するのに十分なものである必要があります。 ただし、このモデルの詳細については、 [ *「回線中心のクォンタム分類子」、「Alex Bocharov、Krysta svore、および Nathan Wiebe」* の提案をお読みください。](https://arxiv.org/abs/1804.00633)
